---
title: "Lesson 13: Exploring Data with the Tidyverse"
author: "Gabriel Odom "
date: "10/14/2019"
date-modified: "2023-11-30"
format:
  html:
    toc: true
    toc-depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE) # 
```

# Review
What have we learned so far this semester? 

1. Writing reports with Quarto
2. Plots with `ggplot2`
3. Vectors
4. Importing and wrangling data with `readr`, `tibble`, and `dplyr`
5. Manipulating strings with `stringr`
5. Writing functions and applying them with `purrr`

</br>

*******************************************************************************
</br>



# Overview
In this lesson, we will combine all of the skills we have learned so far to explore data with the Tidyverse.
```{r, message=FALSE}
library(tidyverse)
```

</br>

*******************************************************************************
</br>



# Importing Data
We will work through some of the American Community Survey data from the U.S. Census Bureau. If you have not yet done so, download the ACS Supplemental Nutrition Assistance Program (SNAP) benefits data ("FOOD STAMPS/Supplemental Nutrition Assistance Program (SNAP) 2012-2016 American Community Survey 5-Year Estimates") for 2021 (use the steps we worked out in Lesson 08).


## The Raw Data
*You should have already done all this in lesson 8.* However, if you did not complete that exercise, download the data and metadata from GitHub. You should have a metadata file called `ACSST5Y2021.S2201-Column-Metadata.csv` which explains all the variable names and a data file called `ACSST5Y2021.S2201-Data.csv` that contains some of the features of interest.


## Read the Data
Now that you have raw data files, import them into R (you will have to update the path to these data sets based on how you saved these files on your computer). ACS data files often have more than one row of column names (the first row is a unique ID for the column, and sometimes the next 1-2 rows are metadata about the column). In our case, we have character information we don't want at the tops of our columns, so we will import the column names first, then import the data.

### SNAP Data Values
We first define a path to our data values (remember to update this to match your machine), then we import only the column headers for this data (that's the `n_max = 0` part). After we have the names we want, we can import the rest of the data---skipping the first two rows---but using the names we calculated in the step before.
```{r read-values-data}
data_path <- "../_data/ACSST5Y2021_S2201/ACSST5Y2021.S2201-Data.csv"
colNames_char <- names(read_csv(data_path, n_max = 0))

FLallZIP_df <- read_csv(
  file = data_path,
  col_names = colNames_char,
  skip = 2
)
```

The meta-data doesn't need those tricks. We can import it as is.
```{r read-metadata}
FLmetadata_df <- read_csv(
  file = "../_data/ACSST5Y2021_S2201/ACSST5Y2021.S2201-Column-Metadata.csv"
)
```

### Inspect Data Sets
Let's get an idea of what is in these data sets:
```{r}
FLallZIP_df
```

There are 457 columns of information (459 - 2 index columns), and it looks like all of these columns have bizarre codes for their names. Also, many of the columns are numeric and many are character (some columns that are character shouldn't be).

Now for the metadata:
```{r}
FLmetadata_df
```
The first column of this data looks exactly like the crazy column names of the main data set. This information should allow us to find out what the column codes mean.


### Updated Summary Function
Recall our lesson on functions last week. Based on this lesson, we are able to write an updated version of the `summary()` function. Our summary function should print the most common unique values for character information, but call the `summary()` function as normal for anything else:
```{r}
MySummary <- function(x){
  
  if (is.character(x)){
    
    table(x) %>% 
      sort(decreasing = TRUE) %>%
      head()
    
  } else {
    summary(x)
  }
  
}
```

### Apply to the Raw Data
Now that we have an updated function to take summaries of a tibble across the columns, let's apply it to each column of the data (because this will print almost 500 results, we will to a random set of 50 columns):
```{r}
set.seed(12345)
# The first two columns are ZIP codes and geography IDs
randColumns_idx <- sort(sample(3:ncol(FLallZIP_df), size = 50))

map(FLallZIP_df[, randColumns_idx], MySummary)
```

It looks like many of these columns (such as `S2201_C02_002M` or `S2201_C06_022E`) have numeric data stored as character information, probably because the symbols `"-"` and `"**"` are used in some columns (they may mark missing data).

If we were planning to perform an analysis of all these features, then we should write code to go through these columns to 1) replace the `"-"` and `"**"` with `NA_real_`, and 2) transform these columns to numeric. However, we only need a few columns from this data set. If the columns we need have this issue, then we will fix it.

</br>

*******************************************************************************
</br>



# REVIEW: Basic String Functions
Because the column name look-up table and many of the data columns contain character information, we need to review a few basic functions from the `stringr` package (included in the `tidyverse`). 


## Detecting Strings
From looking at the columns in the metadata table, it would probably be helpful to find if a word or phrase is contained in a character string. This is the function of `str_detect()`.
```{r}
FLmetadata_df %>% 
  pull(Label) %>% 
  str_detect(pattern = "Hispanic") %>% 
  table()
```

As we can see, this function returns a logical vector indicating if the string `"Hispanic"` is in the elements of the `Label` column, then the `table()` function tells us this information is present in only a handful of the columns. By itself, this isn't helpful: we don't know what the strings are, and we certainly don't know which rows of the tibble they belong to. But at least we know that we have some data on Hispanic households.

To solve these problems, we combine `str_detect()` with `mutate()` and `filter()`:
```{r}
FLmetadata_df %>% 
  mutate(Hispanic_logi = str_detect(Label, "Hispanic")) %>% 
  filter(Hispanic_logi) %>% 
  select(-Hispanic_logi)
```

This is a very nice result: we used the `mutate()` function to add a column of `TRUE` or `FALSE` values to the data frame to indicate if `"Hispanic"` was present in the column names (in the `Label` column). Then, we selected all of the rows where this indicator was `TRUE`, and then removed the indicator (because we no longer needed it).

::: {.callout-note title="Exercises"}
## Exercises
1. Try to find "HISPANIC" and "hispanic". What can we do to modify the metadata so that one search will find all the rows we might want?
2. Many of the rows in the meta-data are not going to be any use to use at all. Can you brainstorm a `stringr::` pipeline that will clean up the meta-data table to make it easier for us to go through?
:::


## String Cases
Because R is case sensitive, the string `"Hispanic"` and `"HISPANIC"` are not the same. We can use the `str_to_upper()` and `str_to_lower()` functions to convert all of the string data to the same case (so our searches don't fail). In this case, we have to make sure to save the new tibble:
```{r}
FLmetadata2_df <- 
  FLmetadata_df %>% 
  mutate(Label = str_to_lower(Label))

# Did it work?
FLmetadata2_df %>% 
  mutate(Hispanic_logi = str_detect(Label, "hispanic")) %>% 
  filter(Hispanic_logi) %>% 
  select(-Hispanic_logi)
```

One tip for working with strings: DO NOT write the new object into the container holding the old object. Strings are incredibly difficult to work with at times, so make sure you can always go back a few steps. This is why I saved the new look-up table with a different name (personally, I usually append a number to the name of the object when I'm working with strings; then, when I'm sure I have done all the string clean-up I need, I save it with a better name).

In the dictionary/look-up table, we have all the information necessary to find the columns we want. However, before we dig into the look-up table, we notice that all of the rows are copied twice: one for estimate, and one for margin of error. For now, we aren't interested in the margins of error, so let's remove these rows:
```{r}
FLmetadata3_df <- 
  FLmetadata2_df %>% 
  mutate(MOE = str_detect(Label, "margin of error")) %>% 
  filter(!MOE) %>% 
  select(-MOE)

# Check
FLmetadata3_df
```

::: {.callout-note title="Exercises"}
## Exercises
1. The meta-data table has information on whether the number is an estimate of the raw count (`total!!`) or an estimate of the population proportion (`percent!!`). Use `stringr::` functions to remove this information from the "Label" column and add it to its own column.
2. Can you break the "Label" column into any other pieces without losing information?
:::

</br>

*******************************************************************************
</br>



# Sanity Checks
Often we have some idea of a pre-existing relationship in the data; e.g, we know that there should be a strong negative relationship between food stamp use and household income within a ZIP code. If our data does not show something we know to be true, then we check that we have the right data and that we are reading the data dictionary correctly.


## Find the "Food Stamps" Metric
We will use the same process as above (`mutate()` + `str_detect()` + `filter()`) to find all the columns that measure food stamp usage:
```{r}
FLmetadata3_df %>% 
  mutate(FoodStamps_logi = str_detect(Id, "food stamps")) %>% 
  filter(FoodStamps_logi) %>% 
  select(-FoodStamps_logi)
```

While there are 152 options, it looks like we want the second one:
```{r}
foodStampsName_char <- 
  FLmetadata3_df %>%  
  mutate(FoodStamps_logi = str_detect(Id, "food stamps")) %>% 
  filter(FoodStamps_logi) %>% 
  slice(2) %>%
  pull(Id)

# Check
foodStampsName_char
```

Now we can find the matching GeoID:
```{r}
foodStampsID_char <- 
  FLmetadata3_df %>% 
  filter(Id == foodStampsName_char) %>% 
  pull(GEO.id)

# Check
foodStampsID_char
```


## Find the "Income" Metric
Just like we found the column name of the food stamps metric we needed, we can find the column name for income:
```{r}
FLmetadata3_df %>%  
  mutate(Income_logi = str_detect(Id, "income")) %>% 
  filter(Income_logi) %>% 
  select(-Income_logi)
```

These labels are too long, so we need to "pull" them out of the data frame:
```{r}
FLmetadata3_df %>%  
  mutate(Income_logi = str_detect(Id, "income")) %>% 
  filter(Income_logi) %>% 
  select(-Income_logi) %>% 
  pull(Id)
```

Because our sanity check is "ZIP codes with higher income should have lower percentages of food stamp recipients", we want the most basic income measurement (the first one).
```{r}
incomeName_char <- 
  FLmetadata3_df %>%  
  mutate(Income_logi = str_detect(Id, "income")) %>% 
  filter(Income_logi) %>% 
  slice(1) %>%
  pull(Id)

# Check
incomeName_char
```

Now we can find the matching GeoID:
```{r}
incomeID_char <- 
  FLmetadata3_df %>% 
  filter(Id == incomeName_char) %>% 
  pull(GEO.id)

# Check
incomeID_char
```


## Subset and Mutate the Data
Now that we have the column names for the data we need, we can extract and inspect those two columns:
```{r}
FLallZIP_df %>% 
  select(incomeID_char, foodStampsID_char) 
```

It looks like have  numeric information stored in character columns. For atomic data, we can follow the type conversion rules to demote this character information to numeric:
```{r}
# Save the results
incomeSNAP_df <- 
  FLallZIP_df %>% 
  # Select and rename the columns we want
  select(income = incomeID_char, foodStamps = foodStampsID_char) %>% 
  # drop character to numeric
  mutate(income = as.numeric(income), foodStamps = as.numeric(foodStamps))
```

Notice that the `"-"` symbols were replaced by `NA` values. (*If you have other features to control for or to add to the plot, make sure you include them here.*)


## Plotting the Relationship
Finally, we have data we can plot! 
```{r, warning=FALSE}
ggplot(data = incomeSNAP_df) +
  aes(x = income, y = foodStamps) +
  geom_point()
```

This clearly shows a negative relationship between income and proportion of residents on food stamps. However, the x-axis shows a strong tail, so let's try to clean this up with a log transformation (also, the relationship looks non-linear, so I've added a LOESS estimate in blue):
```{r, warning=FALSE, message=FALSE}
ggplot(data = incomeSNAP_df) +
  aes(x = log10(income), y = foodStamps) +
  geom_point(alpha = 0.3) +
  geom_smooth(se = FALSE)
```

In practice, at this point we would thoroughly inspect all of the features of interest, *and* plot their relationships with the outcomes of interest. However, we don't have time to do this in class.



> Exercises:
>
> 1. Import the ZIP code data for Miami-Dade and Broward counties.
> 2. Merge these two county tibbles, and clean up the ZIP code column (if you haven't already done this as part of your homework from Lesson 08).
> 3. Use a `*_join()` function to create a single subset of the 2016 ACS SNAP data set for these two counties.

> Exercise: re-plot this relationship for Miami-Dade and Broward counties (make sure to use the same scales for proper comparison). Do you see any differences between the relationship between food stamp usage and median income for the ZIP codes in our counties vs the whole state?

</br>

*******************************************************************************
</br>



# Generating Hypotheses
Based on the above graph, there is a strong relationship between income and the proportion of food stamp recipients in a ZIP code, but it isn't perfect. There are clearly some other factors. This is where generating hypotheses comes in.

> Exercises:
> 
> 1. Work in groups of 4 to generate 8-10 hypotheses to explain the excess variability in the relationship between income and percentage of food stamp recipients within a ZIP code. Include at least 2 multiple-predictor hypotheses (y depends on x within/interacting with z).
> 2. Use string detection to find out if any of the columns of the data are related to the driving factors in your hypotheses. If you do not have data measured to graphically test a hypothesis, remove it from your list (but keep a record of it). Make sure you still have 4-5 hypotheses left (if you don't, go back to exercise 1).
> 3. Graphically test the remaining hypotheses.
> 4. Write a report outlining the hypotheses that you wanted to try but could not (because of lack of data), the hypotheses you tried, the corresponding graphs, and your explinations of each.

